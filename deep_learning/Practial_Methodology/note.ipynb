{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load /Users/facai/Study/book_notes/preconfig.py\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font='SimHei', font_scale=2.5)\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "def show_image(filename, figsize=None, res_dir=True):\n",
    "    if figsize:\n",
    "        plt.figure(figsize=figsize)\n",
    "\n",
    "    if res_dir:\n",
    "        filename = './res/{}'.format(filename)\n",
    "\n",
    "    plt.imshow(plt.imread(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 Practical Methodology\n",
    "========================\n",
    "\n",
    "practical design process:\n",
    "\n",
    "1. Determine your goals.\n",
    "   + which error metric to use?\n",
    "     - coverage: \n",
    "   + what level of performance you desire?\n",
    "2. Establish a working end-to-end pipeline as soon as possible.\n",
    "3. Instrument the system well to determine bottlenecks in performance.\n",
    "4. Repeatedly make incremental changes.\n",
    "   + Determinig whether to gather more data <=  when train error is accpted, while test error not.\n",
    "     - plot curves showing the relationship between training set size and generalization error. usually logarithmic scale.\n",
    "   + Selecting Hyperparameters\n",
    "     - manually: learning rate\n",
    "     - automatic\n",
    "       1. grid search: typically use logarithmic scale\n",
    "       2. random search: more convenient to use, and converges much faster\n",
    "   + Debugging strategies\n",
    "     1. Visualize the model in action.\n",
    "     2. Visualize the worst mistakes.\n",
    "     3. Reasoning about software using train and test error.\n",
    "     4. Fit a tiny dataset.\n",
    "     5. Compare back-propagated derivatives to numerical derivatives.\n",
    "     6. Monitor histograms of activations and gradient:\n",
    "        + visualize statistics of neural network activations and gradients, collected over a large amount of training iterations (maybe one epoch).\n",
    "        + compare the magnitude of parameter gradients to themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
