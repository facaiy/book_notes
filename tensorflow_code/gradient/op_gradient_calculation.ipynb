{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load /Users/facai/Study/book_notes/preconfig.py\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 梯度函数\n",
    "=========\n",
    "\n",
    "版本信息：remotes/upstream/r1.8\n",
    "```\n",
    "~/W/g/t/t/cc ❯❯❯ git log -n 1                                                      \n",
    "commit 8753e2ebde6c58b56675cc19ab7ff83072824a62 (HEAD, upstream/r1.8)\n",
    "Author: Yifei Feng <1192265+yifeif@users.noreply.github.com>\n",
    "Date:   Fri Apr 27 17:05:02 2018 -0700\n",
    "\n",
    "    Fixing the mock import error for devel docker. (#18940)\n",
    "\n",
    "    * Fixing the mock import error for devel docker.\n",
    "\n",
    "    Same as #18843\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 常见位置\n",
    "\n",
    "\n",
    "梯度函数在Python侧和C++侧，均有独立的注册机制。\n",
    "\n",
    "+ 对于Python侧，一般在 `tensorflow/python/ops/*_grad.py`。\n",
    "\n",
    "+ 对于C++侧，有两个独立的注册位置：\n",
    "  - `cc/gradients/*_grad.cc`: 这里是和python对应的移植实现，层级简单。\n",
    "  - `core/ops/*_grad.cc`: 这里似乎是为Function准备的，我对它了解不多，层级很复杂，目前似乎是实验状态，暂不讲，以后清晰了再说。    \n",
    "     注: 这个接口有意思的地方是，它的输入输出是强类型，挺好。\n",
    "\n",
    "\n",
    "下面我画了个简单的类层级关系，用`AbsGrad`梯度为例，有些地方不严谨，作示意导航用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "Image('./res/tf_gradients.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们一般是在python端调用`tf.gradients`显式计算梯度。这个方法会从子图的末端算子往前回溯，并同时查询每个算子注册的梯度，串接起来。所以，在TensorFlow中，使用的都是解析梯度，而只有在梯度单元测试中，才会用数值梯度来做校验。详细说明可见[does-tensorflow-use-automatic-or-symbolic-gradients](https://stackoverflow.com/questions/36370129/does-tensorflow-use-automatic-or-symbolic-gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 如何写Gradient函数\n",
    "\n",
    "官方有说明文档，见 [Implement the gradient in Python](https://www.tensorflow.org/extend/adding_an_op#implement_the_gradient_in_pythoN) 。这里就不赘述，举几个实际例子方便理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 SquareGrad\n",
    "\n",
    "第一类是最常见的实现形式，在Python和C++各自独立实现同样逻辑。\n",
    "\n",
    "以平方算子为例，$y = x ^ 2$，它的梯度是$dy / dx = 2x$。反向传播时要带上后级梯度，所以结果是$dL / dx = \\text{grad} * 2 x$。\n",
    "\n",
    "+ Python侧代码如下：\n",
    "\n",
    "    ```python\n",
    "    @ops.RegisterGradient(\"Square\")\n",
    "    def _SquareGrad(op, grad):\n",
    "      x = op.inputs[0]\n",
    "      # Added control dependencies to prevent 2*x from being computed too early.\n",
    "      with ops.control_dependencies([grad]):\n",
    "        x = math_ops.conj(x)\n",
    "        y = constant_op.constant(2.0, dtype=x.dtype)\n",
    "        return math_ops.multiply(grad, math_ops.multiply(x, y))\n",
    "    ```\n",
    "\n",
    "+ C++侧代码如下：\n",
    "\n",
    "    ```cpp\n",
    "    Status SquareGrad(const Scope& scope, const Operation& op,\n",
    "                      const std::vector<Output>& grad_inputs,\n",
    "                      std::vector<Output>* grad_outputs) {\n",
    "      // dy/dx = (2 * x)\n",
    "      auto two = Cast(scope, Const(scope, 2), op.input(0).type());\n",
    "      auto dydx = Mul(scope, two, op.input(0));\n",
    "      // grad(x) = grad(y) * conj(dy/dx)\n",
    "      grad_outputs->push_back(\n",
    "          Mul(scope, grad_inputs[0], ConjugateHelper(scope, dydx)));\n",
    "      return scope.status();\n",
    "    }\n",
    "    REGISTER_GRADIENT_OP(\"Square\", SquareGrad);\n",
    "    ```\n",
    "    \n",
    "    附上些简要说明： \n",
    "    + `op.input`和`op.output`是`Square`输入x和输出y = x^2。\n",
    "    + `grad_inputs[0]`是反向传导的后级导数`dy`，其实是`dL/dy`的简写。\n",
    "    + 这个函数要计算`dx`，是`dy/dx`的简写, 结果是2 x。\n",
    "    + 返回结果写入`grad_outputs`里，`dL/dx = dL/dy * dy/dx`。\n",
    "\n",
    "    如果输入有多个`x = (x0, x1, ..., xn)`，要分别算对应的导数`[dL/dx0, dL/dx1, ... dL/dxn]`。\n",
    "\n",
    "\n",
    "代码比较简单，唯一会让人困惑的点，可能是我们如何知道`op.input`, `op.output`和`grad_inputs`各自对应的具体参数，下面细说下。\n",
    "\n",
    "##### 关于`op.input`和`op.output`\n",
    "\n",
    "其实这里输入输出对应的是算子的c++接口定义，一般我们可以在它的API定义(core/api_def/base_api/*.pbtxt)或者算子注册文件(core/ops/*.cc)里找到定义。更简单的方法，是调用程序，直接输出算子的定义，如下：\n",
    "\n",
    "```python\n",
    "In [28]: b = tf.square(a)     \n",
    "\n",
    "In [29]: b.op.op_def\n",
    "Out[29]:\n",
    "name: \"Square\"\n",
    "input_arg {\n",
    "  name: \"x\"\n",
    "  type_attr: \"T\"\n",
    "}\n",
    "output_arg {\n",
    "  name: \"y\"\n",
    "  type_attr: \"T\"\n",
    "}\n",
    "attr {\n",
    "  name: \"T\"\n",
    "  type: \"type\"\n",
    "  allowed_values {\n",
    "    list {\n",
    "      type: DT_HALF\n",
    "      type: DT_BFLOAT16\n",
    "      type: DT_FLOAT\n",
    "      type: DT_DOUBLE\n",
    "      type: DT_INT32\n",
    "      type: DT_INT64\n",
    "      type: DT_COMPLEX64\n",
    "      type: DT_COMPLEX128\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "我们可以看到，对于Square算子，它只有一个输入$x$，和一个输出$y$，所以它的`op.input(0) = x`, `op.output(0) = y`。\n",
    "\n",
    "\n",
    "##### 关于`grad_inputs`\n",
    "\n",
    "这里其实是对应于后级输出`op.output`，有几个输出，每个输出就有一个梯度返回。对于Square算子，它只有一个输出，所以也只有一个`grad_inputs(0) = dL / dy`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 SqrtGrad\n",
    "\n",
    "第二类梯度是为了运算效率加速，把梯度本身实现成一个单独的算子，然后在C++和Python端调用同样的底层算子。\n",
    "\n",
    "以开方为例，$y = \\sqrt(x)$，它的梯度是$dy / dx = \\frac1{2} x ^{- \\frac1{2}} = 0.5 / y$。所以完整结果是$dL / dx = 0.5 * \\text{grad} / y$。\n",
    "\n",
    "这个算子首先在eigen名字空间下实现了梯度逻辑的kernel：\n",
    "```bash\n",
    "/core/kernels/cwise_ops.h: 含有Eigen扩展：functor和numext\n",
    "/core/kernels/cwise_ops_gradients.h: 导数\n",
    "/core/kernels/cwise_op_sqrt.cc: 注册核函数\n",
    "```\n",
    "\n",
    "```cpp\n",
    "// Gradient for the sqrt function\n",
    "template <typename T>\n",
    "struct scalar_sqrt_gradient_op {\n",
    "  EIGEN_EMPTY_STRUCT_CTOR(scalar_sqrt_gradient_op)\n",
    "  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const T\n",
    "  operator()(const T& output, const T& output_gradient) const {\n",
    "    const T out_conj = numext::conj(output);\n",
    "    return static_cast<T>(0.5) * output_gradient / out_conj;\n",
    "  }\n",
    "  template <typename Packet>\n",
    "  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet\n",
    "  packetOp(const Packet& output, const Packet& output_gradient) const {\n",
    "    const Packet const_half = pset1<Packet>(static_cast<T>(0.5));\n",
    "    const Packet out_conj = pconj(output);\n",
    "    return pdiv(pmul(const_half, output_gradient), out_conj);\n",
    "  }\n",
    "};\n",
    "```\n",
    "\n",
    "然后在`/core/ops/math_ops.cc`下注册接口：\n",
    "\n",
    "```cpp\n",
    "REGISTER_OP(\"SqrtGrad\").UNARY_GRADIENT_COMPLEX();\n",
    "```\n",
    "\n",
    "接下来在梯度注册代码处，调用这个算子就可以：\n",
    "\n",
    "+ Python端调用\n",
    "    ```python\n",
    "    @ops.RegisterGradient(\"Sqrt\")\n",
    "    def _SqrtGrad(op, grad):\n",
    "      y = op.outputs[0]  # y = x^(1/2)\n",
    "      return gen_math_ops.sqrt_grad(y, grad)\n",
    "    ```\n",
    "  \n",
    "+ C++端调用\n",
    "\n",
    "    ```cpp\n",
    "    Status SqrtGrad(const Scope& scope, const Operation& op,\n",
    "                    const std::vector<Output>& grad_inputs,\n",
    "                    std::vector<Output>* grad_outputs) {\n",
    "      // Use the built-in operator.\n",
    "      grad_outputs->push_back(\n",
    "          internal::SqrtGrad(scope, op.output(0), grad_inputs[0]));\n",
    "      return scope.status();\n",
    "    }\n",
    "    REGISTER_GRADIENT_OP(\"Sqrt\", SqrtGrad);\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 SqrtGradGrad\n",
    "\n",
    "第三类是一些名字如`xxxGradGrad`的梯度算子。因为反向传播是利用的一阶导的链式法式，我们应该把它单纯当作是`xxxGrad`公式展开的一阶导，而不要把它视为`xxx`的二阶导，否则会带来很多困惑。\n",
    "\n",
    "以`SqrtGradGrad`算子为例，它旳实际计算式是$dy = 0.5 * \\text{grad} / y$，有两个输入grad和$y$，所以它有两个导数：\n",
    "\n",
    "为了和代码符号一致，我们改写实际计算式为$y = 0.5 * b / a$：\n",
    "\n",
    "\\begin{align}\n",
    "    dL / da &= \\text{grad} * 0.5 * b * -1 * a^{-2} \\\\\n",
    "            &= - \\text{grad} * 0.5 * b * / a / a \\\\\n",
    "            &= - \\text{grad} * y / a \\\\\n",
    "            &= - \\text{grad} / a * y \\\\\n",
    "            &= - \\text{ga} * y \\\\\n",
    "    dL / db &= \\text{grad} * 0.5 / a \\\\\n",
    "            &= 0.5 * \\text{ga} \\\\\n",
    "\\end{align}\n",
    "\n",
    "所以Python侧实现代码是：\n",
    "\n",
    "```python\n",
    "@ops.RegisterGradient(\"SqrtGrad\")\n",
    "def _SqrtGradGrad(op, grad):\n",
    "  a = op.inputs[0]\n",
    "  y = op.outputs[0]  # y = 0.5 * b / conj(a)\n",
    "  with ops.control_dependencies([grad]):\n",
    "    ga = grad / a\n",
    "    return -math_ops.conj(ga) * y, 0.5 * ga\n",
    "```\n",
    "\n",
    "强调，实际计算式一定是和具体实现对应的，要把`xxxGrad`理解成一个算子，它有实际输入和输出，而不要把它认为是`xxx`的附属。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 小结\n",
    "\n",
    "本文简要介绍了TensorFlow的梯度相关架构，并给出几个具体梯度实现的分析例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
