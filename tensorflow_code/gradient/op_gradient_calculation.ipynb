{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 梯度函数\n",
    "=========\n",
    "\n",
    "版本信息：\n",
    "```\n",
    "~/W/tensorflow git:study/tf_gradient ❯❯❯ git log -n 1\n",
    "commit b1f83c977f3865bb3ec2c1ede97f35ef3cda08d9\n",
    "Merge: 0c1ea2d aeb799d\n",
    "Author: Qianli Scott Zhu <scottzhu@google.com>\n",
    "Date:   Fri Apr 13 19:23:23 2018 -0700\n",
    "\n",
    "    Merge pull request #18511 from qlzh727/branch_192842670\n",
    "\n",
    "    Branch 192842670\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 常见位置\n",
    "\n",
    "算子的梯度函数，可以在Python侧或者C++侧创建，目前看到的趋势是，有大量的Python实现正在往C++迁移。\n",
    "\n",
    "对于Python侧的梯度函数实现，一般在 `tensorflow/python/ops` 目录下，以 `_grad` 后缀结尾，如 `array_grad.py`。\n",
    "\n",
    "对于C++侧的梯度函数实在，\n",
    "\n",
    "+ kernel:\n",
    "+ op: cc/gradient: 泛的接口\n",
    "+ core/ops: 静态接口\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 如何写Gradient函数\n",
    "\n",
    "+ python侧\n",
    "+ c++侧\n",
    "\n",
    "拿Square作例子，细讲Grad和GradGrad的具体推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "kernels/cwise_ops.h: 含有Eigen扩展：functor和numext\n",
    "kernels/cwise_ops_gradients.h: 导数\n",
    "```\n",
    "\n",
    "\n",
    "### Gradient\n",
    "\n",
    "```cpp\n",
    "Status SquareGrad(const Scope& scope, const Operation& op,\n",
    "                  const std::vector<Output>& grad_inputs,\n",
    "                  std::vector<Output>* grad_outputs) {\n",
    "  // dy/dx = (2 * x)\n",
    "  auto two = Cast(scope, Const(scope, 2), op.input(0).type());\n",
    "  auto dydx = Mul(scope, two, op.input(0));\n",
    "  // grad(x) = grad(y) * conj(dy/dx)\n",
    "  grad_outputs->push_back(\n",
    "      Mul(scope, grad_inputs[0], ConjugateHelper(scope, dydx)));\n",
    "  return scope.status();\n",
    "}\n",
    "REGISTER_GRADIENT_OP(\"Square\", SquareGrad);\n",
    "```\n",
    "\n",
    "+ `op.input`和`op.output`是`Square`输入x和输出y = x^2。\n",
    "+ `grad_inputs[0]`是反向传导的后级导数`dy`，其实是`dL/dy`的简写。\n",
    "+ 这个函数要计算`dx`，是`dy/dx`的简写, 结果是2 x。\n",
    "+ 返回结果写入`grad_outputs`里，`dL/dx = dL/dy * dy/dx`。\n",
    "\n",
    "如果输入有多个`x = (x0, x1, ..., xn)`，要分别算对应的导数`[dL/dx0, dL/dx1, ... dL/dxn]`。\n",
    "\n",
    "\n",
    "注意：对于XXXGradGrad后缀的导数算子，将它当作XXXGrad的一阶导处理，并非是从损失过来的二阶导数。不要以为这是二阶导的反向传导。\n",
    "\n",
    "具体见手写笔记。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 梯度注册与查找"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "反向推导使用的是gradient function，先用python注册，没有再去c++那查。\n",
    "只在TestCase中用数值方法来检查实现。\n",
    "\n",
    "https://stackoverflow.com/questions/36370129/does-tensorflow-use-automatic-or-symbolic-gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
