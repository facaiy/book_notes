{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 梯度相关内容梳理\n",
    "=========\n",
    "\n",
    "版本信息：\n",
    "```\n",
    "~/W/g/tensorflow ❯❯❯ git log -n 1\n",
    "commit f7acdf2ed5e0b9c50c1c5f4b80163255aa9e8073 (HEAD -> study/tf_gradient, upstream/master, master)\n",
    "Merge: 7558b085af 7d52acd62b\n",
    "Author: Akshay Modi <akshaym@users.noreply.github.com>\n",
    "Date:   Mon Mar 5 15:51:26 2018 -0800\n",
    "\n",
    "    Merge pull request #17449 from akshaym/branch_187892975\n",
    "\n",
    "    Branch 187892975\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 常见Gradient位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 如何写Gradient函数\n",
    "\n",
    "+ python侧\n",
    "+ c++侧\n",
    "\n",
    "拿Square作例子，细讲Grad和GradGrad的具体推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 梯度注册"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 反向求导与参数更新\n",
    "\n",
    "Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "kernels/cwise_ops.h: 含有Eigen扩展：functor和numext\n",
    "kernels/cwise_ops_gradients.h: 导数\n",
    "```\n",
    "\n",
    "\n",
    "### Gradient\n",
    "\n",
    "```cpp\n",
    "Status SquareGrad(const Scope& scope, const Operation& op,\n",
    "                  const std::vector<Output>& grad_inputs,\n",
    "                  std::vector<Output>* grad_outputs) {\n",
    "  // dy/dx = (2 * x)\n",
    "  auto two = Cast(scope, Const(scope, 2), op.input(0).type());\n",
    "  auto dydx = Mul(scope, two, op.input(0));\n",
    "  // grad(x) = grad(y) * conj(dy/dx)\n",
    "  grad_outputs->push_back(\n",
    "      Mul(scope, grad_inputs[0], ConjugateHelper(scope, dydx)));\n",
    "  return scope.status();\n",
    "}\n",
    "REGISTER_GRADIENT_OP(\"Square\", SquareGrad);\n",
    "```\n",
    "\n",
    "+ `op.input`和`op.output`是`Square`输入x和输出y = x^2。\n",
    "+ `grad_inputs[0]`是反向传导的后级导数`dy`，其实是`dL/dy`的简写。\n",
    "+ 这个函数要计算`dx`，是`dy/dx`的简写, 结果是2 x。\n",
    "+ 返回结果写入`grad_outputs`里，`dL/dx = dL/dy * dy/dx`。\n",
    "\n",
    "如果输入有多个`x = (x0, x1, ..., xn)`，要分别算对应的导数`[dL/dx0, dL/dx1, ... dL/dxn]`。\n",
    "\n",
    "\n",
    "注意：对于XXXGradGrad后缀的导数算子，将它当作XXXGrad的一阶导处理，并非是从损失过来的二阶导数。不要以为这是二阶导的反向传导。\n",
    "\n",
    "具体见手写笔记。\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "反向推导使用的是gradient function，先用python注册，没有再去c++那查。\n",
    "只在TestCase中用数值方法来检查实现。\n",
    "\n",
    "https://stackoverflow.com/questions/36370129/does-tensorflow-use-automatic-or-symbolic-gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
