% -*- coding: utf-8 -*-

\section{Gradient Boost Decision Tree (GBDT)}
% http://nbviewer.jupyter.org/github/facaiy/book_notes/blob/master/machine_learning/tree/gbdt/intro.ipynb
% http://nbviewer.jupyter.org/github/facaiy/book_notes/blob/master/machine_learning/tree/gbdt/spark/intro.ipynb
\subsection{直观印象}
% 模型叠加
% 残差
\begin{frame}
    \begin{figure}[!tb]
        \includegraphics[width=1.2\onepicwidth]{figure/gbdt/OurMethodv81}
        \caption{GBDT示意\footnote{
                 \href{http://mprg.jp/research/boostedrandomforest_e}{Boosted Random Forest and Transfer Learning}}}
    \end{figure}
\end{frame}


\subsection{算法流程}
% https://en.wikipedia.org/wiki/Gradient_boosting
%% greedy: pdf, Algorithm 1
\begin{frame}
    \begin{algorithm}[H]
        $F_0(x) = \operatorname{arg \, min}_\rho \sum_{i=1}^N L(y_i, \rho)$ \;
        \For{$m=1$ \KwTo $M$}{
            $\tilde{y} = - \left [ \frac{\partial L (y_i, F(x_i))}{\partial F(x_i)} \right ]_{F(x) = F_{m-1}(x)}, \quad i = 1, 2, \dotsc, N$ \;
            $\mathbf{a}_m = \operatorname{arg \, min}_{\mathbf{a}, \beta} \sum_{i=1}^N \left [ \tilde{y}_i - \beta h(x_i; \mathbf{a}) \right ]^2$ \;
            $\rho_m = \operatorname{arg \, min}_\rho \sum_{i=1}^N L \left ( y_i, F_{m-1}(x_i) + \rho h(x_i; \mathbf{a}_m) \right)$ \;
            $F_m(x) = F_{m-1}(x) + \rho_m h(x; \mathbf{a}_m)$ \;
        }
        \caption{Gradient\_Boost}
    \end{algorithm}
    {\tiny Greedy function approximation: A gradient boosting machine, Jerome H. Friedman}
\end{frame}


\subsection{从最优化角度的理解}
\begin{frame}
    \begin{figure}[!tb]
        \includegraphics[width=\onepicwidth]{figure/gbdt/Gradient_Visual}
        \caption{损失函数示意\footnote{
                 \href{https://en.wikipedia.org/wiki/File:Gradient_Visual.svg}{Gradient Visual, Wikipedia}}}
    \end{figure}
\end{frame}

% 从最速下降法来理解
%% L(,) 距离函数
%% 一维 U
%% 二维 曲面
\begin{frame}
    \begin{figure}
        \centering
        \subfigure[][]{
            \resizebox{0.53\linewidth}{!}{\input{figure/gbdt/gradient}}
        }
        \hfil
        \subfigure[][]{
            \resizebox{0.40\linewidth}{!}{\input{figure/gbdt/find_route}}
        }
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \resizebox{1.1\onepicwidth}{!}{\input{figure/gbdt/space_map}}
    \end{figure}
\end{frame}

%\begin{frame}
%    \begin{figure}
%        \centering
%        \resizebox{\textwidth}{!}{\input{figure/gbdt/random_forest}}
%    \end{figure}
%\end{frame}


\subsection{从泛函角度的理解}
% 泛函，不是数据在变，而是函数在变
%% 训练是找到函数
%% 预测是找到输出
\begin{frame}
    \begin{columns}
        \begin{column}{.55\textwidth}
            \begin{figure}
                \centering
                \resizebox{0.9\linewidth}{!}{\input{figure/gbdt/basis}}
            \end{figure}
        \end{column}

        \begin{column}{.35\textwidth}
            \begin{figure}
                \includegraphics[width=0.9\linewidth]{figure/gbdt/vector_space_3d}
                \caption{三维向量空间\footnotemark}
            \end{figure}
        \end{column}
    \end{columns}

    \footnotetext{${\mathbf {e}}_{x} = (1,0,0), \quad {\mathbf  {e}}_{y} = (0,1,0), \quad {\mathbf {e}}_{z} = (0,0,1)$}
\end{frame}

% 为什么选择决策树？
% 数据，泰勒展开， 正交基底
%% 加法模型 = 正交基底，函数空间
%\begin{frame}
%    ${\mathbf  {e}}_{x}=(1,0,0),\quad {\mathbf  {e}}_{y}=(0,1,0),\quad {\mathbf {e}}_{z}=(0,0,1)$.
%
%    \begin{figure}[!tb]
%        \includegraphics[width=0.8\onepicwidth]{figure/gbdt/3D_Vector}
%        \caption{向量空间\footnote{
%                 \href{https://en.wikipedia.org/wiki/Standard_basis}{Standard basis, Wikipedia}}}
%    \end{figure}
%\end{frame}

\begin{frame}
    泰勒展开：$\sum _{n=0}^{\infty }{\frac{f^{(n)}(a)}{n!}}\,\textcolor{blue}{(x-a)^{n}}$

    \begin{figure}[!tb]
        \includegraphics[width=\twopicwidth]{figure/gbdt/sine-better-models}
        \caption{sin函数泰勒展开示意\footnote{
                 \href{https://betterexplained.com/articles/intuitive-understanding-of-sine-waves/}{Intuitive Understanding of Sine Waves}}}
    \end{figure}

    GBDT: $F_n(x) = F_{0}(x) + \sum_n \rho_m \textcolor{blue}{h(x; \mathbf{a}_m)}$
\end{frame}


\subsection{从降维角度的理解}
% PCA
%% 数据特征的降维 -> 函数空间的降维
%% 矩阵分解
\begin{frame}
    \begin{figure}
        \centering
        \subfigure[][]{
            \includegraphics[width=0.40\linewidth]{figure/gbdt/PCA}
        }
        \hfil
        \subfigure[][]{
            \resizebox{0.50\linewidth}{!}{\input{figure/gbdt/basis}}
        }
        \caption{PCA\footnotemark 比较示意}
    \end{figure}

    \footnotetext{\href{https://lazyprogrammer.me/tutorial-principal-components-analysis-pca/}{Tutorial: Principal Components Analysis (PCA)}}
\end{frame}


\subsection{spark实现代码}
% 寻优耗时
% spark,实现，没有寻优 => tree boost
%% 用sklearn, spark代码引出Tree Boost
\begin{frame}[fragile]
    \begin{lstlisting}[language=Scala,style=myScalastyle]
def boost(
  // +-- 46 lines: input: RDD[LabeledPoint],------------------------------
  val firstTree = new DecisionTreeRegressor().setSeed(seed)
  val firstTreeModel = firstTree.train(input, treeStrategy)
  val firstTreeWeight = 1.0
  baseLearners(0) = firstTreeModel
  baseLearnerWeights(0) = firstTreeWeight
  // +-- 17 lines: var predError: RDD[(Double, Double)] =-----------------
  while (m < numIterations && !doneLearning) {
    // Update data with pseudo-residuals
    val data = predError.zip(input).map { case ((pred, _), point) =>
    LabeledPoint(-loss.gradient(pred, point.label), point.features)
    }
    // +--  5 lines: timer.start(s"building tree $m")----------------------
    val dt = new DecisionTreeRegressor().setSeed(seed + m)
    val model = dt.train(data, treeStrategy)
    baseLearners(m) = model
    baseLearnerWeights(m) = learningRate

    predError = updatePredictionError(
    input, predError, baseLearnerWeights(m), baseLearners(m), loss)
    // +-- 21 lines: predErrorCheckpointer.update(predError)---------------
    m += 1
  }
}
    \end{lstlisting}
    {\tiny \tt
    source: spark/ml/tree/impl/GradientBoostedTrees.scala \\[-2ex]
    commit: 2eedc00b04ef8ca771ff64c4f834c25f835f5f44}
\end{frame}
